import os
import rclpy
from rclpy.node import Node
from ament_index_python.packages import get_package_share_directory
from dotenv import load_dotenv

# from llm_node_pkg.srv import TriggerLLM
from std_srvs.srv import Trigger

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

############ Package Path & Environment Setting ############
current_dir = os.getcwd()
package_path = get_package_share_directory("llm_for_pick_place_voice")
# env_path = "/home/hun/ws/llm_ws/src/llm_for_pick_place_voice"
is_load = load_dotenv(dotenv_path=os.path.join(f"{package_path}/resource/.env"))
openai_api_key = os.getenv("OPENAI_API_KEY")

class TriggerLLMServer(Node):
    def __init__(self):
        # llm_trigger_server ë…¸ë“œ ìƒì„±
        super().__init__('llm_trigger_server')
        self.srv = self.create_service(Trigger, 'llm_trigger', self.handle_request)
        # ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ ì•Œë¦¼
        self.get_logger().info("ì„œë¹„ìŠ¤ ì„œë²„ ì¤€ë¹„ë¨")


        # LangChain LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o",  # ì›í•˜ëŠ” ëª¨ë¸
            temperature=0.2, # ë‹¤ìŒ ë‹¨ì–´ ì„ íƒì„ ìœ„í•œ í™•ë¥ (0ì¼ ê²½ìš° í™•ë¥  ë†’ì€ ë‹¨ì–´)
            openai_api_key=openai_api_key
        )
        # ğŸ“Œ ë©”ì‹œì§€ í…œí”Œë¦¿ ìƒì„±
        prompt_template = """
        ë‹¹ì‹ ì€ ë›°ì–´ë‚œ í‚¤ì›Œë“œ ì¶”ì¶œê¸°ì…ë‹ˆë‹¤.

        <ì‚¬ìš©ì ì…ë ¥>
        "{user_input}"
        """

        self.prompt = PromptTemplate(
            input_variables=["user_input"], template=prompt_template
        )

        self.chain = self.prompt | self.llm  # ìµœì‹  êµ¬ì¡°

    def handle_request(self, request, response):
        # ì„œë²„ê°€ ì§ì ‘ ì…ë ¥ ë°›ìŒ!
        user_text = input("\nğŸ‘‰ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

        # LLM í‚¤ì›Œë“œ ì¶”ì¶œ
        ai_msg = self.chain.invoke({"user_input": user_text})
        llm_out = ai_msg.content


        # object / target ë‚˜ëˆ„ê¸°
        try:
            obj, tgt = llm_out.strip().split("/")
        except:
            obj, tgt = llm_out, ""

        obj_list = obj.split()

        response.success = True
        response.message = " ".join(obj_list)

        print(f"LLM ê²°ê³¼: {llm_out}")
        print(f"object: {obj_list}")

        return response
    # # 
    # def handle_request(self, request, response):
    #     user_input = request.prompt
    #     self.get_logger().info(f"[LLM] Received prompt: {user_input}")

    #     chain = self.prompt_template | self.llm

    #     llm_result = chain.invoke({"user_prompt": user_input})

    #     response.answer = llm_result.content
    #     return response


# trigger_server.py ì‹¤í–‰
def main(args=None):
    rclpy.init(args=args)
    node = TriggerLLMServer()

    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()


if __name__ == "__main__":
    main()